from google.colab import files
uploaded = files.upload()
import io
import numpy as np
data = np.load(io.BytesIO(uploaded['sdss_galaxy_colors.npy']))
print(data[0])

#data inspection
data['redshift'].shape

#extracting data and splitting them 

features = np.zeros((data.shape[0], 4))
features[:, 0] = data['u'] - data['g']
features.shape

# function to construct our data by feature extraction
def get_features_targets(data):
  features = np.zeros(shape=(len(data), 4))
  features[:, 0] = data['u'] - data['g']
  features[:, 1] = data['g'] - data['r']
  features[:, 2] = data['r'] - data['i']
  features[:, 3] = data['i'] - data['z']
  targets = data['redshift']
  return features, targets

#usage of the function 
features, targets = get_features_targets(data)
#print(features[:2])
#print(targets[:2])

from sklearn.tree import DecisionTreeRegressor

dtr = DecisionTreeRegressor()
#training the model 
dtr.fit(features, targets)
#doing prediction 
predictions = dtr.predict(features)

#prediction inspection 
predictions[0:4]
targets[0:4]

#median residual calculation procedure -- definition of the median function
def median_diff(predicted, actual):
  return np.median(np.abs(predicted[:] - actual[:]))

#usage of the median function
diff = median_diff(predictions, targets)
print("Median difference: {:0.8f}".format(diff))
# This will be exact for prediction because we used the whole data for training and testing 

# splitting the feature data 
split = features.shape[0]//2
train_features = features[:split]
test_features = features[split:]

# defining a function for splitting the dataset
def validate_model(model, features, targets):
  # split the data into training and testing
  split = 2*features.shape[0]//3
  train_features, test_features = features[:split], features[split:]
  train_targets, test_targets = targets[:split], targets[split:]

  # train the model
  model.fit(train_features, train_targets)

  # get the predicted_redshifts
  predictions = model.predict(test_features)  
  
  # use median_diff function to calculate the accuracy
  return median_diff(test_targets, predictions)



#FINAL USAGE
diff = validate_model(dtr, features, targets)
print('Median difference: {:f}'.format(diff))

# Kfold version -- importing the lib
from sklearn.model_selection import KFold

#KFOLD cross validation function definition 
def cross_validate_model(model, features, targets, k):
  kf = KFold(n_splits=k, shuffle=True)

  # initialise a list to collect median_diffs for each iteration of the loop below
  diffs = []

  for train_indices, test_indices in kf.split(features):
    train_features, test_features = features[train_indices], features[test_indices]
    train_targets, test_targets = targets[train_indices], targets[test_indices]
    
    # fit the model for the current set
    model.fit(train_features, train_targets)
    
    # predict using the model
    predictions = model.predict(test_features)
 
    # calculate the median_diff from predicted values and append to results array
    diffs.append(median_diff(predictions, test_targets))
 
  # return the list with your median difference values
  return diffs


dtr = DecisionTreeRegressor(max_depth=19)
diffs = cross_validate_model(dtr, features, targets, 10) # k = 10
print('Differences: {}'.format(', '.join(['{:.4f}'.format(val) for val in diffs])))
print('Mean difference: {:.4f}'.format(np.mean(diffs)))

#USING CROSS VALIDATE PREDICTIONS -- function definition 
def cross_validate_predictions(model, features, targets, k):
  kf = KFold(n_splits=k, shuffle=True)

  # declare an array for predicted redshifts from each iteration
  all_predictions = np.zeros_like(targets)

  for train_indices, test_indices in kf.split(features):
    # split the data into training and testing
    train_features, test_features = features[train_indices], features[test_indices]
    train_targets, test_targets = targets[train_indices], targets[test_indices]
    
    # fit the model for the current set
    model.fit(train_features, train_targets)
        
    # predict using the model
    predictions = model.predict(test_features)
        
    # put the predicted values in the all_predictions array defined above
    all_predictions[test_indices] = predictions

  # return the predictions
  return all_predictions

import matplotlib.pyplot as plt

#usage of cross_validate_predictions
dtr = DecisionTreeRegressor(max_depth=19)
predictions = cross_validate_predictions(dtr, features, targets, 10)

diffs = median_diff(predictions, targets)
print('Median difference: {:.4f}'.format(diffs))
plt.scatter(targets, predictions, s=0.4)
plt.xlim((0, targets.max()))
plt.ylim((0, predictions.max()))
plt.xlabel('Measured Redshift')
plt.ylabel('Predicted Redshift')
plt.show()


#reading the data with pandas 
#import pandas as pd
#median residual calculation procedure -- definition of the median function
def median_diff_separator(predicted, actual, data):
  conditionTest = b'QSO'
  conditionTest1 = b'GALAXY'
  QSOData = data['spec_class'] == conditionTest
  regularGalaxyData = data['spec_class'] == conditionTest1
  # to find the accuracy of the prediction for QSOs and GALAXIES we apply the filter to the diffs data
  QSODiffs = np.median(np.abs(predicted[QSOData] - actual[QSOData])) # mean accuracy of QSOs
  regularGalaxyDiffs = np.median(np.abs(predicted[regularGalaxyData] - actual[regularGalaxyData])) # mean accuracy of regularGALAXIES

  return QSODiffs, regularGalaxyDiffs


### **"whether there is a difference in the accuracy of the decision trees between QSOs and regular galaxies?"**


QSODiffs, regularGalaxyDiffs = median_diff_separator(predictions, targets, data)
print("accuracy of QSO: ", QSODiffs, ", accuracy of regular Galaxies: ", regularGalaxyDiffs)



### **"Additionally, try to find the best fraction of split and test data"**


# we write a for loop to test different splitting portions and calculate the final diffs, finally we decide with the least diffs of the corresponding splitting portion
splitter = range(5, 55, 5)
diffArray = []
for i in splitter:
  dtr = DecisionTreeRegressor(max_depth=19)
  predictions = cross_validate_predictions(dtr, features, targets, i)
  diffArray.append(median_diff(predictions, targets))


# now we find the MIN in diffArray and print the corresponding splitter value
minDiff = np.amin(np.asarray(diffArray))
minIndex = np.where(np.asarray(diffArray) == minDiff)
optimumSplittingSize = (minIndex[0]+1) * 5

print("Optimum splitting size for the current dataset: ", optimumSplittingSize)
